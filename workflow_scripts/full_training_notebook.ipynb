{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is designed to be worked in kaggle, but you may use your local system or other web hosted tools.\n",
    "\n",
    "You will need (tf_requirements.txt in kaggle input dataset named requirements) AND (your updated epic7_match_history.csv, hero_types.csv in kaggle input dataset named data-csvs)\n",
    "\n",
    "You will also need to change code to match file location of epic7_match_history.csv to your correct updated match history csv location.\n",
    "\n",
    "This notebook includes most of the tools needed to train the recommendation model such as merging csvs. You may skip some codeblocks if it is not needed in your scenario (such as merging csvs, downloading github repo, etc), so it is important to understand what the code does.\n",
    "\n",
    "You may need to change some codes to fit your file locations or file hierarchy or you may encounter errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T02:59:54.239556Z",
     "iopub.status.busy": "2025-02-26T02:59:54.239323Z",
     "iopub.status.idle": "2025-02-26T03:01:23.637197Z",
     "shell.execute_reply": "2025-02-26T03:01:23.636338Z",
     "shell.execute_reply.started": "2025-02-26T02:59:54.239529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -r /kaggle/input/requirements/tf_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T03:01:23.639426Z",
     "iopub.status.busy": "2025-02-26T03:01:23.639145Z",
     "iopub.status.idle": "2025-02-26T03:01:24.651752Z",
     "shell.execute_reply": "2025-02-26T03:01:24.650949Z",
     "shell.execute_reply.started": "2025-02-26T03:01:23.639397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!for a in /sys/bus/pci/devices/*; do echo 0 | tee -a $a/numa_node; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T03:01:24.653247Z",
     "iopub.status.busy": "2025-02-26T03:01:24.652987Z",
     "iopub.status.idle": "2025-02-26T03:01:25.679022Z",
     "shell.execute_reply": "2025-02-26T03:01:25.678120Z",
     "shell.execute_reply.started": "2025-02-26T03:01:24.653221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!lspci | grep -i nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T03:01:25.680669Z",
     "iopub.status.busy": "2025-02-26T03:01:25.680289Z",
     "iopub.status.idle": "2025-02-26T03:01:26.674883Z",
     "shell.execute_reply": "2025-02-26T03:01:26.673833Z",
     "shell.execute_reply.started": "2025-02-26T03:01:25.680607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%env TF_USE_LEGACY_KERAS=True\n",
    "!echo $TF_USE_LEGACY_KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**delete git repo and clone it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T03:01:26.677305Z",
     "iopub.status.busy": "2025-02-26T03:01:26.676498Z",
     "iopub.status.idle": "2025-02-26T03:01:28.359470Z",
     "shell.execute_reply": "2025-02-26T03:01:28.358205Z",
     "shell.execute_reply.started": "2025-02-26T03:01:26.677261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf E7-RTA-Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-26T03:01:28.361284Z",
     "iopub.status.busy": "2025-02-26T03:01:28.360983Z",
     "iopub.status.idle": "2025-02-26T03:02:16.482284Z",
     "shell.execute_reply": "2025-02-26T03:02:16.481513Z",
     "shell.execute_reply.started": "2025-02-26T03:01:28.361255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone -b update-data https://github.com/SamTheCoder777/E7-RTA-Helper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-26T03:02:50.550Z",
     "iopub.execute_input": "2025-02-26T03:02:16.484826Z",
     "iopub.status.busy": "2025-02-26T03:02:16.484525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import hashlib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "def merge(csv_files):\n",
    "    merged_csv = pd.DataFrame()\n",
    "    max_match_number = 0\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Read the current CSV file\n",
    "        current_csv = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Adjust match numbers to avoid duplicates\n",
    "        current_csv['Match Number'] += max_match_number\n",
    "        \n",
    "        # Update the max match number for the next iteration\n",
    "        max_match_number = current_csv['Match Number'].max()\n",
    "        \n",
    "        # Concatenate the current CSV to the merged DataFrame\n",
    "        merged_csv = pd.concat([merged_csv, current_csv], ignore_index=True)\n",
    "\n",
    "    # Group by 'Match Number' and remove duplicates\n",
    "    grouped = merged_csv.groupby('Match Number')[['Pick Order', 'Match Result', 'Team', 'Hero', 'First Pick']].agg(tuple)\n",
    "    is_duplicated = grouped.duplicated(keep=\"first\")\n",
    "    \n",
    "    # Get only the non-duplicated 'Match Number' values\n",
    "    unique_match_numbers = grouped[~is_duplicated].index\n",
    "    \n",
    "    # Filter the original DataFrame to retain only the non-duplicated 'Match Number' groups\n",
    "    merged_csv = merged_csv[merged_csv['Match Number'].isin(unique_match_numbers)]\n",
    "    \n",
    "    # Re-factorize the 'Match Number' to be sequential\n",
    "    merged_csv.loc[:, 'Match Number'] = pd.factorize(merged_csv['Match Number'])[0] + 1\n",
    "    return merged_csv\n",
    "\n",
    "# Removing this from the workflow because the file became too large\n",
    "\n",
    "# Get files from the folder\n",
    "csvs = [join('E7-RTA-Helper/match_histories', f) for f in listdir('E7-RTA-Helper/match_histories') if isfile(join('E7-RTA-Helper/match_histories', f))]\n",
    "print(csvs)\n",
    "\n",
    "# Extract numbers from filenames\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else -1  # Default to -1 if no number is found\n",
    "\n",
    "# Sort by extracted number in descending order\n",
    "csvs = sorted(csvs, key=lambda x: extract_number(x), reverse=True)\n",
    "\n",
    "# Get the 20 most recent files\n",
    "csvs = csvs[:20]\n",
    "\n",
    "# Merge all CSVs\n",
    "final_csv = merge(csvs)\n",
    "print(final_csv)\n",
    "# Save the merged DataFrame to a new CSV\n",
    "final_csv.to_csv('E7-RTA-Helper/data/epic7_match_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-26T03:02:50.551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_matches():\n",
    "    # Load the data\n",
    "    matches = pd.read_csv('E7-RTA-Helper/data/epic7_match_history.csv')\n",
    "\n",
    "    # Group by match number\n",
    "    match_group = matches.groupby('Match Number')\n",
    "    invalid_matches = []\n",
    "    for match, group in match_group:\n",
    "        if group['Pick Order'].nunique() != 10:\n",
    "            invalid_matches.append(match)\n",
    "\n",
    "        characters_used = set()\n",
    "        for i in range(1, 11):\n",
    "            pick = group[group['Pick Order'] == i]\n",
    "            if not pick.empty:\n",
    "                current_char = pick['Hero'].values[0]\n",
    "                if current_char in characters_used:\n",
    "                    invalid_matches.append(match)\n",
    "                    break\n",
    "\n",
    "                characters_used.add(current_char)\n",
    "\n",
    "        if group[group['Team'] == 'My Team']['Team'].value_counts().values[0] != 5 or\\\n",
    "                group[group['Team'] == 'Enemy Team']['Team'].value_counts().values[0] != 5:\n",
    "                    invalid_matches.append(match)\n",
    "\n",
    "    #now save the valid groups only\n",
    "    matches = matches[~matches['Match Number'].isin(invalid_matches)]\n",
    "    matches.to_csv('E7-RTA-Helper/data/epic7_match_history.csv', index=False)\n",
    "\n",
    "validate_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-02-26T03:02:50.551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Masking, Concatenate, Input, Lambda\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import activations, initializers, regularizers\n",
    "from tensorflow.keras.layers import LSTM, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from attention import Attention\n",
    "import ast\n",
    "import pickle\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('E7-RTA-Helper/data/epic7_match_history.csv')\n",
    "hero_details = pd.read_csv('/kaggle/input/data-csvs/hero_types.csv')\n",
    "\n",
    "data = data.merge(hero_details, left_on='Hero', right_on='code', how='left')\n",
    "\n",
    "# Add features and encode\n",
    "data['First_Pick_Win'] = (data['First Pick'] == 1) & (data['Match Result'] == 'Win')\n",
    "data['First_Pick_Win_encoded'] = data['First_Pick_Win'].astype(int)\n",
    "\n",
    "# Encode heroes and teams as integers\n",
    "hero_encoder = LabelEncoder()\n",
    "hero_encoder.fit(list(data['Hero'].unique())+['unknown'])\n",
    "data['Hero_encoded'] = hero_encoder.transform(data['Hero'])\n",
    "\n",
    "team_encoder = LabelEncoder()\n",
    "data['Team_encoded'] = team_encoder.fit_transform(data['Team'])\n",
    "\n",
    "# Function to check for problematic entries\n",
    "def ensure_list(x):\n",
    "    if pd.isna(x):\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "# Apply the function to identify rows with issues\n",
    "data['is_valid'] = data['type'].apply(ensure_list)\n",
    "\n",
    "# Identify the Match Numbers with any invalid rows\n",
    "invalid_matches = data.loc[~data['is_valid'], 'Match Number'].unique()\n",
    "print(f'Invalid Matches: {invalid_matches}') # This usually turns out to be matches where 3 star fodders are used\n",
    "\n",
    "# Identify the Match Numbers with any invalid rows\n",
    "invalid_matches = data.loc[~data['is_valid'], 'Match Number'].unique()\n",
    "\n",
    "# Drop all rows with those Match Numbers\n",
    "data = data[~data['Match Number'].isin(invalid_matches)].drop(columns=['is_valid'])\n",
    "\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        print(f\"Error with value: {val}\")\n",
    "        return val  # Return the original value if it fails\n",
    "\n",
    "# Convert the 'type' column from string to list\n",
    "data['type'] = data['type'].apply(safe_literal_eval)\n",
    "\n",
    "# Encode types as multi-hot vectors\n",
    "type_encoder = MultiLabelBinarizer()\n",
    "unique_types = np.unique(data['type'])\n",
    "\n",
    "type_encoder.fit(list(unique_types)+['Unknown'])\n",
    "data['Type_encoded'] = list(type_encoder.transform(data['type']))\n",
    "\n",
    "# Encode first pick\n",
    "first_pick_encoder = LabelEncoder()\n",
    "data['First_Pick_encoded'] = first_pick_encoder.fit_transform(data['First Pick'])\n",
    "\n",
    "# Prepare sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "pick_orders = []\n",
    "team_sequences = []\n",
    "type_sequences = []\n",
    "first_pick_sequences = []\n",
    "first_pick_win_sequences = []\n",
    "\n",
    "for match_number in data['Match Number'].unique():\n",
    "    match_df = data[data['Match Number'] == match_number]\n",
    "    \n",
    "    picks_sequence = match_df.sort_values(by='Pick Order')['Hero_encoded'].tolist()\n",
    "    pick_order_sequence = match_df.sort_values(by='Pick Order')['Pick Order'].tolist()\n",
    "    team_sequence = match_df.sort_values(by='Pick Order')['Team_encoded'].tolist()\n",
    "    type_sequence = match_df.sort_values(by='Pick Order')['Type_encoded'].tolist()\n",
    "    first_pick_sequence = match_df.sort_values(by='Pick Order')['First_Pick_encoded'].tolist()\n",
    "    first_pick_win_sequence = match_df.sort_values(by='Pick Order')['First_Pick_Win_encoded'].tolist()\n",
    "\n",
    "    for i in range(1, len(picks_sequence)):\n",
    "        sequences.append(picks_sequence[:i])\n",
    "        labels.append(picks_sequence[i])\n",
    "        pick_orders.append(pick_order_sequence[:i])\n",
    "        team_sequences.append(team_sequence[:i])\n",
    "        type_sequences.append(type_sequence[:i])\n",
    "        first_pick_sequences.append(first_pick_sequence[:i])\n",
    "        first_pick_win_sequences.append(first_pick_win_sequence[:i])\n",
    "\n",
    "unique_type_values = np.unique(np.concatenate(type_sequences))\n",
    "print(\"Unique values in type_sequences before padding:\", unique_type_values)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "print(f\"Max sequence length: {max_sequence_length}\")\n",
    "X_heroes = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')\n",
    "X_pick_orders = pad_sequences(pick_orders, maxlen=max_sequence_length, padding='pre')\n",
    "X_teams = pad_sequences(team_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "X_first_picks = pad_sequences(first_pick_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "# Convert lists to numpy arrays and pad sequences\n",
    "X_first_pick_wins = pad_sequences(first_pick_win_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Convert lists of multi-hot vectors to numpy array\n",
    "X_types = pad_sequences([np.array(x) for x in type_sequences], maxlen=max_sequence_length, padding='pre', dtype='float32')\n",
    "\n",
    "y = to_categorical(labels, num_classes=len(hero_encoder.classes_))\n",
    "y_win = np.array([sequence[0] for sequence in first_pick_win_sequences])\n",
    "\n",
    "# Parameters\n",
    "num_heroes = len(hero_encoder.classes_)\n",
    "num_types = len(type_encoder.classes_)\n",
    "embedding_dim = 512 \n",
    "lstm_units = 256 \n",
    "\n",
    "# Preprocessing\n",
    "X_heroes = np.clip(X_heroes, 0, num_heroes - 1)\n",
    "X_pick_orders = np.clip(X_pick_orders, 0, max_sequence_length - 1)\n",
    "X_teams = np.clip(X_teams, 0, len(team_encoder.classes_) - 1)\n",
    "X_first_picks = np.clip(X_first_picks, 0, len(first_pick_encoder.classes_) - 1)\n",
    "X_first_pick_wins = np.clip(X_first_pick_wins, 0, max_sequence_length - 1)\n",
    "\n",
    "# Build the model\n",
    "input_heroes = Input(shape=(max_sequence_length,))\n",
    "input_pick_orders = Input(shape=(max_sequence_length,))\n",
    "input_teams = Input(shape=(max_sequence_length,))\n",
    "input_types = Input(shape=(max_sequence_length, num_types))\n",
    "input_first_picks = Input(shape=(max_sequence_length,))\n",
    "# Define input layers\n",
    "input_first_pick_wins = Input(shape=(max_sequence_length,))\n",
    "\n",
    "hero_embedding = Embedding(input_dim=num_heroes, output_dim=embedding_dim, input_length=max_sequence_length)(input_heroes)\n",
    "masking_heroes = Masking(mask_value=0.0)(hero_embedding)\n",
    "\n",
    "pick_order_embedding = Embedding(input_dim=max_sequence_length, output_dim=embedding_dim, input_length=max_sequence_length)(input_pick_orders)\n",
    "masking_orders = Masking(mask_value=0.0)(pick_order_embedding)\n",
    "\n",
    "team_embedding = Embedding(input_dim=len(team_encoder.classes_), output_dim=embedding_dim, input_length=max_sequence_length)(input_teams)\n",
    "masking_teams = Masking(mask_value=0.0)(team_embedding)\n",
    "\n",
    "first_pick_embedding = Embedding(input_dim=len(first_pick_encoder.classes_), output_dim=embedding_dim, input_length=max_sequence_length)(input_first_picks)\n",
    "masking_first_picks = Masking(mask_value=0.0)(first_pick_embedding)\n",
    "\n",
    "# Define embeddings for the new inputs\n",
    "first_pick_win_embedding = Embedding(input_dim=2, output_dim=embedding_dim, input_length=max_sequence_length)(input_first_pick_wins)\n",
    "# Apply masking\n",
    "masking_first_pick_wins = Masking(mask_value=0.0)(first_pick_win_embedding)\n",
    "\n",
    "# Apply Masking before Dense layer for types\n",
    "masking_types = Masking(mask_value=0.0)(input_types)\n",
    "type_embedding = Dense(embedding_dim, activation='relu')(masking_types)\n",
    "#masking_types = Masking(mask_value=0.0)(type_embedding)\n",
    "\n",
    "# Concatenate all inputs\n",
    "concatenated = Concatenate()([masking_heroes, masking_orders, masking_teams, masking_first_picks, type_embedding, masking_first_pick_wins])\n",
    "concatenated_win = Concatenate()([masking_heroes, masking_orders, masking_teams, masking_first_picks, type_embedding])\n",
    "def check_indices(data, max_index):\n",
    "    if np.any(data >= max_index):\n",
    "        print(f\"Error: Found indices out of range in data. Max index allowed: {max_index - 1}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Check indices for each input array\n",
    "if not check_indices(X_heroes, num_heroes):\n",
    "    print(\"Invalid indices in X_heroes\")\n",
    "if not check_indices(X_pick_orders, max_sequence_length):\n",
    "    print(\"Invalid indices in X_pick_orders\")\n",
    "if not check_indices(X_teams, len(team_encoder.classes_)):\n",
    "    print(\"Invalid indices in X_teams\")\n",
    "if not check_indices(X_first_picks, len(first_pick_encoder.classes_)):\n",
    "    print(\"Invalid indices in X_first_picks\")\n",
    "if not check_indices(X_types, num_types):\n",
    "    print(\"Invalid indices in X_types\")\n",
    "\n",
    "\n",
    "lstm_out1 = LSTM(512, return_sequences=True)(concatenated)\n",
    "attention = Attention(name='attention_weight')(lstm_out1)\n",
    "output = Dense(num_heroes, activation='softmax')(attention)\n",
    "\n",
    "lstm_out2 = LSTM(128, return_sequences=True)(concatenated_win)\n",
    "attention_win = Attention(name='attention_win_weight')(lstm_out2)\n",
    "win_hidden = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(attention_win)\n",
    "win_hidden = Dropout(0.5)(win_hidden)\n",
    "win_output = Dense(1, activation='sigmoid', name='win_output')(win_hidden)\n",
    "\n",
    "model = Model([input_heroes, input_pick_orders, input_teams, input_first_picks, input_types, input_first_pick_wins], [output, win_output])\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "    \n",
    "model.compile(loss={'dense_1': 'categorical_crossentropy', 'win_output': 'binary_crossentropy'},\n",
    "               optimizer=optimizer, \n",
    "              metrics={'dense_1':['accuracy',\n",
    "                       tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'),\n",
    "                       tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy'),\n",
    "                       tf.keras.metrics.TopKCategoricalAccuracy(k=10, name='top_10_accuracy')],\n",
    "                       'win_output':['accuracy']})\n",
    "    \n",
    "# Callbacks\n",
    "checkpoint_filepath = 'rec_model.h5'\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=0)\n",
    "#learning_rate_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "# Save encoders\n",
    "with open('rec_variables.pkl', 'wb') as f:\n",
    "    pickle.dump([type_encoder, hero_encoder, max_sequence_length], f)\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "history = model.fit([X_heroes, X_pick_orders, X_teams, X_first_picks, X_types, X_first_pick_wins], \n",
    "                    {'dense_1': y, 'win_output': y_win}, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[model_checkpoint_callback, early_stopping])\n",
    "\n",
    "# Save model without optimizer\n",
    "model = tf.keras.models.load_model('rec_model.h5', custom_objects={'Attention': Attention})\n",
    "keras.models.save_model(model, \"rec_model.h5\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-26T03:02:50.551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf E7-RTA-Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-26T03:02:50.551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pkill jupyter"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5780882,
     "sourceId": 9499369,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5781137,
     "sourceId": 9499693,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5781164,
     "sourceId": 10973574,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
